<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Oira.AI</title>
    <link rel="icon" href="media/logo.webp">
    <meta name="description" content="Oira, AI, Oira.AI">
    <style>
      @font-face {
        font-family: "Fragmentcore";
        src: url("fonts/Fragmentcore.otf");
      }

      @font-face {
        font-family: "Roboto";
        src: url("fonts/Roboto-Regular.ttf");
      }

      ::-webkit-scrollbar {
        width: 11px;
      }

      ::-webkit-scrollbar-track {
        border: solid 3px transparent;
      }

      ::-webkit-scrollbar-thumb {
        box-shadow: inset 0 0 10px 10px white;
        border: solid 3px transparent;
        border-radius: 25px;
      }

      body {
        margin: 0;
        background-color: #000;
      }

      h1 {
        color: #fff;
      }

      main {
        width: 100%;
      }

      .tech {
        display: flex;
        flex-direction: column;
        justify-content: flex-start;
        font-family: "Roboto", arial, sans-serif;
        color: #fff;
        padding: 20px;
      }

      .MNB, .VITS {
        width: 50dvw;
        margin: auto;
        height: auto;
        margin-bottom: 50px;
      }

      img {
        width: 100%;
        margin: 10px 0px;
      }

      li {
        margin-bottom: 20px;
      }

      @media (max-width: 900px) {
        .MNB, .VITS {
          width: 85dvw;
          margin: auto;
          height: auto;
          margin-bottom: 50px;
      }
      }
    </style>
  </head>
  <body>
    <main>
        <div class="tech">
		<div class="MNB">
        <h1 style="text-align: center;">Multinomial Naive Bayes</h1>
        <h3>What is the Multinomial Naive Bayes algorithm?</h3>
        <p>The Multinomial Naive Bayes algorithm is a probabilistic learning method widely applied in Natural Language Processing (NLP). It leverages the principles of the Bayes theorem to predict the class (e.g., language) of a text document, email, or other textual data. It calculates the probability distribution of each class for a given document and assigns the class with the highest probability as the prediction.</p>
        <h3>How does it work?</h3>
        <p>The Multinomial Naive Bayes algorithm is specifically designed for text classification tasks. Formulated based on the Bayes theorem, originally proposed by Thomas Bayes, it computes the probability of an event occurring given the prior knowledge of conditions associated with the event. The algorithm is governed by the following formula:</p>
        <p><b><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} 
        </annotation></semantics></math></b></p>
        <p>Here, we evaluate the probability of class A given the presence of predictor B.</p>
        <ul>
        <li><b>P(B)</b>: Prior probability of predictor B</li>
        <li><b>P(A)</b>: Prior probability of class A</li>
        <li><b>P(B|A)</b>: Conditional probability of predictor B given class A</li>
        </ul>
        <img draggable="false" src="media/Multinomial Naive Bayes - Visual.webp">
        <p>The image likely includes a visual representation of the Naive Bayes Classifier at work. It might show data points, represented as shapes or colors, being classified into different categories.</p>
        <p>The “naive” aspect comes from the assumption that all features are independent of each other within each class. Despite this simplification, Naive Bayes Classifiers work well in many real-world situations, especially for text classification and spam filtering.</p>
        <p>
          All in all, the Multinomial Naive Bayes algorithm is a powerful tool for text classification in Natural Language Processing. Leveraging the Bayes theorem, it calculates the probability distribution of each class for a given text document and assigns the class with the highest probability as the prediction. Despite its "naive" assumption of feature independence within each class, it proves effective in real-world scenarios, particularly in tasks like text classification and spam filtering. Its simplicity and efficiency make it a popular choice for various NLP applications.</p>
		  </div>
      <div class="VITS">
        <h1 style="text-align: center;">VITS</h1>
        <h3>What is VITS?</h3>
        <p>Vits, also known as "Variational Inference with adversarial
          learning for end-to-end Text-to-Speech", is a novel approach in the field of natural language processing and speech synthesis. It aims to improve the quality and naturalness of synthesized speech by employing variational inference techniques along with adversarial learning.</p>
          <p>VITS is an end-to-end speech synthesis model that predicts a speech waveform conditional on an input text sequence. It is a conditional variational autoencoder (VAE) comprised of a posterior encoder, decoder, and conditional prior.</p>
          <h3>How does it work?</h3>
          <p>VITS works by leveraging variational inference and adversarial learning techniques to generate high-quality speech from input text. Here's a brief overview of its functioning:</p>
          <ol>
              <li><b>Text Encoding:</b> The input text sequence is encoded using a posterior encoder, which captures the latent representation of the text.</li>
              <li><b>Variational Sampling:</b> VITS employs variational inference to sample from a distribution in the latent space, allowing for stochasticity in the generation process.</li>
              <li><b>Conditional Waveform Generation:</b> The sampled latent representation is combined with the conditioning information (such as speaker identity or prosody features) to generate the speech waveform.</li>
              <li><b>Adversarial Training:</b> Adversarial learning is employed to further refine the generated speech, making it more natural and indistinguishable from real speech samples.</li>
              <li><b>End-to-End Training:</b> The entire model, including the encoder, decoder, and adversarial components, is trained jointly in an end-to-end fashion to optimize for speech quality.</li>
          </ol>
          <p>By integrating these components, VITS achieves state-of-the-art performance in text-to-speech synthesis, producing speech that is both natural-sounding and highly intelligible.</p>
          <img draggable="false" src="media/VITS - Visual.webp">
          <p>The image is a diagram explaining the processes involved in a voice synthesis system, specifically using a model known as VITS (Variational Inference for Text-to-Speech). The diagram is divided into two main parts:</p>
          <ol>
            <li><b>Training procedure:</b> This flowchart shows how raw audio waveforms are processed to train the model. It involves slicing the waveform, encoding it, searching for monotonic alignment, and then projecting it to generate text phonemes. There are feedback loops and parameters indicated by Greek letters (like θ and σ), which are typical in machine learning models.</li>
            <li><b>Inference procedure:</b> This part of the diagram illustrates how the trained model takes a new raw waveform and predicts the duration of phonemes, which are then used to generate the corresponding text output.</li>
          </ol>
          <p>The process involves complex algorithms and machine learning techniques to transform spoken words into written text.</p>
        <p>All in all, VITS represents a cutting-edge advancement in the field of text-to-speech synthesis, combining variational inference and adversarial learning to generate natural-sounding speech from input text sequences.</p>
        </div>
        </div>
    </main>
    </body>
</html>